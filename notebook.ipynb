{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler,OneHotEncoder\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"bigdata-project_withSparkML\").getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/E:/third year/Big_Data_Tech/Project/Disease_prediction/data/data.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#'hdfs://localhost:9000/usr/baonguyen/data.csv'\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mthird year\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mBig_Data_Tech\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProject\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDisease_prediction\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/E:/third year/Big_Data_Tech/Project/Disease_prediction/data/data.csv."
     ]
    }
   ],
   "source": [
    "#'hdfs://localhost:9000/usr/baonguyen/data.csv'\n",
    "df = spark.read.csv(r'E:\\third year\\Big_Data_Tech\\Project\\Disease_prediction\\data\\datatest.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+-------------------+---------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+------------------+-------------------+--------------------+--------------------+--------------------+------------------+-------------------+-------------------+--------------------+-------------------+--------------------+--------------------+-------------------+-------------------+----------------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+---------------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------------------+------------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+-------------------------+--------------------+--------------------+--------------------+------------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+--------------------+---------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+---------------------------+------------------------------+--------------------+--------------------+---------------------+------------------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+\n",
      "|summary|            itching|          skin_rash|nodal_skin_eruptions| continuous_sneezing|          shivering|             chills|         joint_pain|        stomach_pain|             acidity|   ulcers_on_tongue|     muscle_wasting|           vomiting|burning_micturition|spotting_ urination|            fatigue|         weight_gain|             anxiety|cold_hands_and_feets|         mood_swings|        weight_loss|        restlessness|           lethargy|  patches_in_throat|irregular_sugar_level|              cough|         high_fever|        sunken_eyes|     breathlessness|           sweating|        dehydration|         indigestion|           headache|     yellowish_skin|         dark_urine|            nausea|   loss_of_appetite|pain_behind_the_eyes|           back_pain|        constipation|    abdominal_pain|          diarrhoea|         mild_fever|        yellow_urine|  yellowing_of_eyes| acute_liver_failure| swelling_of_stomach|swelled_lymph_nodes|            malaise|blurred_and_distorted_vision|             phlegm|   throat_irritation|     redness_of_eyes|      sinus_pressure|          runny_nose|          congestion|         chest_pain|  weakness_in_limbs|    fast_heart_rate|pain_during_bowel_movements| pain_in_anal_region|        bloody_stool|  irritation_in_anus|           neck_pain|          dizziness|              cramps|            bruising|             obesity|        swollen_legs|swollen_blood_vessels| puffy_face_and_eyes|    enlarged_thyroid|       brittle_nails| swollen_extremeties|   excessive_hunger|extra_marital_contacts|drying_and_tingling_lips|      slurred_speech|           knee_pain|      hip_joint_pain|    muscle_weakness|          stiff_neck|     swelling_joints|  movement_stiffness| spinning_movements|    loss_of_balance|        unsteadiness|weakness_of_one_body_side|       loss_of_smell|  bladder_discomfort| foul_smell_of urine|continuous_feel_of_urine|    passage_of_gases|    internal_itching| toxic_look_(typhos)|         depression|       irritability|        muscle_pain|   altered_sensorium|red_spots_over_body|          belly_pain|abnormal_menstruation|dischromic _patches| watering_from_eyes|  increased_appetite|            polyuria|      family_history|       mucoid_sputum|        rusty_sputum|lack_of_concentration| visual_disturbances|receiving_blood_transfusion|receiving_unsterile_injections|                coma|    stomach_bleeding|distention_of_abdomen|history_of_alcohol_consumption|      fluid_overload|     blood_in_sputum|prominent_veins_on_calf|        palpitations|     painful_walking| pus_filled_pimples|         blackheads|           scurring|        skin_peeling| silver_like_dusting|small_dents_in_nails|  inflammatory_nails|             blister|red_sore_around_nose|   yellow_crust_ooze|           prognosis|Unnamed: 133|\n",
      "+-------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+-------------------+---------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+------------------+-------------------+--------------------+--------------------+--------------------+------------------+-------------------+-------------------+--------------------+-------------------+--------------------+--------------------+-------------------+-------------------+----------------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+---------------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------------------+------------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+-------------------------+--------------------+--------------------+--------------------+------------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+--------------------+---------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+---------------------------+------------------------------+--------------------+--------------------+---------------------+------------------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+\n",
      "|  count|               4920|               4920|                4920|                4920|               4920|               4920|               4920|                4920|                4920|               4920|               4920|               4920|               4920|               4920|               4920|                4920|                4920|                4920|                4920|               4920|                4920|               4920|               4920|                 4920|               4920|               4920|               4920|               4920|               4920|               4920|                4920|               4920|               4920|               4920|              4920|               4920|                4920|                4920|                4920|              4920|               4920|               4920|                4920|               4920|                4920|                4920|               4920|               4920|                        4920|               4920|                4920|                4920|                4920|                4920|                4920|               4920|               4920|               4920|                       4920|                4920|                4920|                4920|                4920|               4920|                4920|                4920|                4920|                4920|                 4920|                4920|                4920|                4920|                4920|               4920|                  4920|                    4920|                4920|                4920|                4920|               4920|                4920|                4920|                4920|               4920|               4920|                4920|                     4920|                4920|                4920|                4920|                    4920|                4920|                4920|                4920|               4920|               4920|               4920|                4920|               4920|                4920|                 4920|               4920|               4920|                4920|                4920|                4920|                4920|                4920|                 4920|                4920|                       4920|                          4920|                4920|                4920|                 4920|                          4920|                4920|                4920|                   4920|                4920|                4920|               4920|               4920|               4920|                4920|                4920|                4920|                4920|                4920|                4920|                4920|                4920|           0|\n",
      "|   mean| 0.1378048780487805| 0.1597560975609756| 0.02195121951219512|0.045121951219512194|0.02195121951219512|0.16219512195121952|0.13902439024390245|0.045121951219512194|0.045121951219512194|0.02195121951219512|0.02195121951219512|0.38902439024390245|0.04390243902439024|0.02195121951219512| 0.3926829268292683|0.023170731707317073|0.023170731707317073|0.023170731707317073|0.046341463414634146|0.09268292682926829|0.046341463414634146|0.09268292682926829|0.02195121951219512| 0.023170731707317073|0.11463414634146342|0.27682926829268295|0.02195121951219512|0.09146341463414634| 0.1378048780487805|0.02195121951219512|0.045121951219512194| 0.2304878048780488|0.18536585365853658|0.11585365853658537|0.2329268292682927|0.23414634146341465|0.024390243902439025|0.046341463414634146|0.046341463414634146|0.2097560975609756|0.11463414634146342|0.07195121951219512|0.023170731707317073|0.16585365853658537|0.023170731707317073|0.023170731707317073|0.07073170731707316|0.14268292682926828|         0.06951219512195123|0.07195121951219512|0.024390243902439025|0.024390243902439025|0.024390243902439025|0.024390243902439025|0.024390243902439025|0.14146341463414633|0.02195121951219512| 0.0475609756097561|       0.023170731707317073|0.023170731707317073|0.023170731707317073|0.023170731707317073|0.046341463414634146|0.06829268292682927|0.023170731707317073|0.023170731707317073|0.046341463414634146|0.023170731707317073|  0.02195121951219512|0.023170731707317073|0.024390243902439025|0.024390243902439025|0.024390243902439025|0.09390243902439024|   0.02195121951219512|    0.023170731707317073|0.024390243902439025|0.023170731707317073|0.023170731707317073| 0.0475609756097561|0.046341463414634146|0.046341463414634146|0.023170731707317073|0.02195121951219512|0.06951219512195123|0.023170731707317073|      0.02195121951219512|0.024390243902439025|0.023170731707317073|0.020731707317073172|    0.023170731707317073|0.023170731707317073|0.023170731707317073|0.023170731707317073| 0.0475609756097561|0.09634146341463415|0.09634146341463415|0.023170731707317073| 0.0475609756097561|0.023170731707317073|  0.04878048780487805|0.02195121951219512|0.02195121951219512|0.024390243902439025|0.024390243902439025|0.046341463414634146|0.023170731707317073|0.024390243902439025| 0.023170731707317073|0.023170731707317073|       0.024390243902439025|          0.024390243902439025|0.024390243902439025|0.024390243902439025| 0.023170731707317073|          0.023170731707317073|0.023170731707317073|0.024390243902439025|   0.023170731707317073|0.024390243902439025|0.046341463414634146|0.02195121951219512|0.02195121951219512|0.02195121951219512|0.023170731707317073|0.023170731707317073|0.023170731707317073|0.023170731707317073|0.023170731707317073|0.023170731707317073|0.023170731707317073|                null|        null|\n",
      "| stddev|0.34473010874032145|0.36641694249935314| 0.14653916900608888| 0.20759267768168482| 0.1465391690060889|  0.368667179805761|0.34600714253632103| 0.20759267768168493| 0.20759267768168482| 0.1465391690060889| 0.1465391690060888| 0.4875784385107517|0.20489887289486605|0.14653916900608885|0.48839689579676626| 0.15046079293451486| 0.15046079293451484| 0.15046079293451486| 0.21024489652747674|0.29001706400009114|  0.2102448965274766| 0.2900170640000912| 0.1465391690060888|  0.15046079293451495|0.31861229059551816| 0.4474768405026227| 0.1465391690060888|0.28829629144304947|0.34473010874032084| 0.1465391690060888| 0.20759267768168482|0.42118788377078326|0.38863357078435745| 0.3200818832755925|0.4227389789091996|0.42350712766036314| 0.15427312579740848| 0.21024489652747674| 0.21024489652747674|0.4071758522965833|0.31861229059551877|0.25843338848204184|   0.150460792934515|0.37198702578714116| 0.15046079293451495| 0.15046079293451486| 0.2564022135464877|0.34978475807309095|          0.2543489707392176|  0.258433388482042| 0.15427312579740846| 0.15427312579740846| 0.15427312579740846| 0.15427312579740846| 0.15427312579740846|0.34853437024233136|0.14653916900608877|0.21285708393186986|        0.15046079293451495| 0.15046079293451495| 0.15046079293451495| 0.15046079293451495| 0.21024489652747655| 0.2522731212317158| 0.15046079293451486| 0.15046079293451484| 0.21024489652747666| 0.15046079293451486|  0.14653916900608902| 0.15046079293451486| 0.15427312579740846| 0.15427312579740846| 0.15427312579740846|0.29172258763760767|   0.14653916900608882|     0.15046079293451484| 0.15427312579740846| 0.15046079293451484| 0.15046079293451484|0.21285708393187003| 0.21024489652747655|  0.2102448965274768|  0.1504607929345148|0.14653916900608896|0.25434897073921753| 0.15046079293451484|      0.14653916900608885| 0.15427312579740846| 0.15046079293451486| 0.14249923112367766|     0.15046079293451486|   0.150460792934515|   0.150460792934515| 0.15046079293451495|0.21285708393186978| 0.2950889434009276| 0.2950889434009279| 0.15046079293451495|0.21285708393186972| 0.15046079293451495|  0.21543069609375737|0.14653916900608888| 0.1465391690060889| 0.15427312579740865| 0.15427312579740865|  0.2102448965274769| 0.15046079293451498| 0.15427312579740837|  0.15046079293451498|   0.150460792934515|        0.15427312579740848|           0.15427312579740848| 0.15427312579740843| 0.15427312579740843|  0.15046079293451486|           0.15046079293451486|  0.1504607929345149| 0.15427312579740837|    0.15046079293451484| 0.15427312579740846|  0.2102448965274768|0.14653916900608893|0.14653916900608893|0.14653916900608893| 0.15046079293451484|  0.1504607929345148| 0.15046079293451484| 0.15046079293451484| 0.15046079293451478| 0.15046079293451478| 0.15046079293451478|                null|        null|\n",
      "|    min|                  0|                  0|                   0|                   0|                  0|                  0|                  0|                   0|                   0|                  0|                  0|                  0|                  0|                  0|                  0|                   0|                   0|                   0|                   0|                  0|                   0|                  0|                  0|                    0|                  0|                  0|                  0|                  0|                  0|                  0|                   0|                  0|                  0|                  0|                 0|                  0|                   0|                   0|                   0|                 0|                  0|                  0|                   0|                  0|                   0|                   0|                  0|                  0|                           0|                  0|                   0|                   0|                   0|                   0|                   0|                  0|                  0|                  0|                          0|                   0|                   0|                   0|                   0|                  0|                   0|                   0|                   0|                   0|                    0|                   0|                   0|                   0|                   0|                  0|                     0|                       0|                   0|                   0|                   0|                  0|                   0|                   0|                   0|                  0|                  0|                   0|                        0|                   0|                   0|                   0|                       0|                   0|                   0|                   0|                  0|                  0|                  0|                   0|                  0|                   0|                    0|                  0|                  0|                   0|                   0|                   0|                   0|                   0|                    0|                   0|                          0|                             0|                   0|                   0|                    0|                             0|                   0|                   0|                      0|                   0|                   0|                  0|                  0|                  0|                   0|                   0|                   0|                   0|                   0|                   0|                   0|(vertigo) Paroyms...|        null|\n",
      "|    max|                  1|                  1|                   1|                   1|                  1|                  1|                  1|                   1|                   1|                  1|                  1|                  1|                  1|                  1|                  1|                   1|                   1|                   1|                   1|                  1|                   1|                  1|                  1|                    1|                  1|                  1|                  1|                  1|                  1|                  1|                   1|                  1|                  1|                  1|                 1|                  1|                   1|                   1|                   1|                 1|                  1|                  1|                   1|                  1|                   1|                   1|                  1|                  1|                           1|                  1|                   1|                   1|                   1|                   1|                   1|                  1|                  1|                  1|                          1|                   1|                   1|                   1|                   1|                  1|                   1|                   1|                   1|                   1|                    1|                   1|                   1|                   1|                   1|                  1|                     1|                       1|                   1|                   1|                   1|                  1|                   1|                   1|                   1|                  1|                  1|                   1|                        1|                   1|                   1|                   1|                       1|                   1|                   1|                   1|                  1|                  1|                  1|                   1|                  1|                   1|                    1|                  1|                  1|                   1|                   1|                   1|                   1|                   1|                    1|                   1|                          1|                             1|                   1|                   1|                    1|                             1|                   1|                   1|                      1|                   1|                   1|                  1|                  1|                  1|                   1|                   1|                   1|                   1|                   1|                   1|                   1|         hepatitis A|        null|\n",
      "+-------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+-------------------+---------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+------------------+-------------------+--------------------+--------------------+--------------------+------------------+-------------------+-------------------+--------------------+-------------------+--------------------+--------------------+-------------------+-------------------+----------------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+---------------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------+--------------------+-------------------+----------------------+------------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+-------------------------+--------------------+--------------------+--------------------+------------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+--------------------+---------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+---------------------------+------------------------------+--------------------+--------------------+---------------------+------------------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4920,\n",
       " ['itching',\n",
       "  'skin_rash',\n",
       "  'nodal_skin_eruptions',\n",
       "  'continuous_sneezing',\n",
       "  'shivering',\n",
       "  'chills',\n",
       "  'joint_pain',\n",
       "  'stomach_pain',\n",
       "  'acidity',\n",
       "  'ulcers_on_tongue',\n",
       "  'muscle_wasting',\n",
       "  'vomiting',\n",
       "  'burning_micturition',\n",
       "  'spotting_ urination',\n",
       "  'fatigue',\n",
       "  'weight_gain',\n",
       "  'anxiety',\n",
       "  'cold_hands_and_feets',\n",
       "  'mood_swings',\n",
       "  'weight_loss',\n",
       "  'restlessness',\n",
       "  'lethargy',\n",
       "  'patches_in_throat',\n",
       "  'irregular_sugar_level',\n",
       "  'cough',\n",
       "  'high_fever',\n",
       "  'sunken_eyes',\n",
       "  'breathlessness',\n",
       "  'sweating',\n",
       "  'dehydration',\n",
       "  'indigestion',\n",
       "  'headache',\n",
       "  'yellowish_skin',\n",
       "  'dark_urine',\n",
       "  'nausea',\n",
       "  'loss_of_appetite',\n",
       "  'pain_behind_the_eyes',\n",
       "  'back_pain',\n",
       "  'constipation',\n",
       "  'abdominal_pain',\n",
       "  'diarrhoea',\n",
       "  'mild_fever',\n",
       "  'yellow_urine',\n",
       "  'yellowing_of_eyes',\n",
       "  'acute_liver_failure',\n",
       "  'swelling_of_stomach',\n",
       "  'swelled_lymph_nodes',\n",
       "  'malaise',\n",
       "  'blurred_and_distorted_vision',\n",
       "  'phlegm',\n",
       "  'throat_irritation',\n",
       "  'redness_of_eyes',\n",
       "  'sinus_pressure',\n",
       "  'runny_nose',\n",
       "  'congestion',\n",
       "  'chest_pain',\n",
       "  'weakness_in_limbs',\n",
       "  'fast_heart_rate',\n",
       "  'pain_during_bowel_movements',\n",
       "  'pain_in_anal_region',\n",
       "  'bloody_stool',\n",
       "  'irritation_in_anus',\n",
       "  'neck_pain',\n",
       "  'dizziness',\n",
       "  'cramps',\n",
       "  'bruising',\n",
       "  'obesity',\n",
       "  'swollen_legs',\n",
       "  'swollen_blood_vessels',\n",
       "  'puffy_face_and_eyes',\n",
       "  'enlarged_thyroid',\n",
       "  'brittle_nails',\n",
       "  'swollen_extremeties',\n",
       "  'excessive_hunger',\n",
       "  'extra_marital_contacts',\n",
       "  'drying_and_tingling_lips',\n",
       "  'slurred_speech',\n",
       "  'knee_pain',\n",
       "  'hip_joint_pain',\n",
       "  'muscle_weakness',\n",
       "  'stiff_neck',\n",
       "  'swelling_joints',\n",
       "  'movement_stiffness',\n",
       "  'spinning_movements',\n",
       "  'loss_of_balance',\n",
       "  'unsteadiness',\n",
       "  'weakness_of_one_body_side',\n",
       "  'loss_of_smell',\n",
       "  'bladder_discomfort',\n",
       "  'foul_smell_of urine',\n",
       "  'continuous_feel_of_urine',\n",
       "  'passage_of_gases',\n",
       "  'internal_itching',\n",
       "  'toxic_look_(typhos)',\n",
       "  'depression',\n",
       "  'irritability',\n",
       "  'muscle_pain',\n",
       "  'altered_sensorium',\n",
       "  'red_spots_over_body',\n",
       "  'belly_pain',\n",
       "  'abnormal_menstruation',\n",
       "  'dischromic _patches',\n",
       "  'watering_from_eyes',\n",
       "  'increased_appetite',\n",
       "  'polyuria',\n",
       "  'family_history',\n",
       "  'mucoid_sputum',\n",
       "  'rusty_sputum',\n",
       "  'lack_of_concentration',\n",
       "  'visual_disturbances',\n",
       "  'receiving_blood_transfusion',\n",
       "  'receiving_unsterile_injections',\n",
       "  'coma',\n",
       "  'stomach_bleeding',\n",
       "  'distention_of_abdomen',\n",
       "  'history_of_alcohol_consumption',\n",
       "  'fluid_overload',\n",
       "  'blood_in_sputum',\n",
       "  'prominent_veins_on_calf',\n",
       "  'palpitations',\n",
       "  'painful_walking',\n",
       "  'pus_filled_pimples',\n",
       "  'blackheads',\n",
       "  'scurring',\n",
       "  'skin_peeling',\n",
       "  'silver_like_dusting',\n",
       "  'small_dents_in_nails',\n",
       "  'inflammatory_nails',\n",
       "  'blister',\n",
       "  'red_sore_around_nose',\n",
       "  'yellow_crust_ooze',\n",
       "  'prognosis',\n",
       "  'Unnamed: 133'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() , df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['Unnamed: 133']\n",
      "+-------+---------+--------------------+-------------------+---------+------+----------+------------+-------+----------------+--------------+--------+-------------------+-------------------+-------+-----------+-------+--------------------+-----------+-----------+------------+--------+-----------------+---------------------+-----+----------+-----------+--------------+--------+-----------+-----------+--------+--------------+----------+------+----------------+--------------------+---------+------------+--------------+---------+----------+------------+-----------------+-------------------+-------------------+-------------------+-------+----------------------------+------+-----------------+---------------+--------------+----------+----------+----------+-----------------+---------------+---------------------------+-------------------+------------+------------------+---------+---------+------+--------+-------+------------+---------------------+-------------------+----------------+-------------+-------------------+----------------+----------------------+------------------------+--------------+---------+--------------+---------------+----------+---------------+------------------+------------------+---------------+------------+-------------------------+-------------+------------------+-------------------+------------------------+----------------+----------------+-------------------+----------+------------+-----------+-----------------+-------------------+----------+---------------------+-------------------+------------------+------------------+--------+--------------+-------------+------------+---------------------+-------------------+---------------------------+------------------------------+----+----------------+---------------------+------------------------------+--------------+---------------+-----------------------+------------+---------------+------------------+----------+--------+------------+-------------------+--------------------+------------------+-------+--------------------+-----------------+----------------+\n",
      "|itching|skin_rash|nodal_skin_eruptions|continuous_sneezing|shivering|chills|joint_pain|stomach_pain|acidity|ulcers_on_tongue|muscle_wasting|vomiting|burning_micturition|spotting_ urination|fatigue|weight_gain|anxiety|cold_hands_and_feets|mood_swings|weight_loss|restlessness|lethargy|patches_in_throat|irregular_sugar_level|cough|high_fever|sunken_eyes|breathlessness|sweating|dehydration|indigestion|headache|yellowish_skin|dark_urine|nausea|loss_of_appetite|pain_behind_the_eyes|back_pain|constipation|abdominal_pain|diarrhoea|mild_fever|yellow_urine|yellowing_of_eyes|acute_liver_failure|swelling_of_stomach|swelled_lymph_nodes|malaise|blurred_and_distorted_vision|phlegm|throat_irritation|redness_of_eyes|sinus_pressure|runny_nose|congestion|chest_pain|weakness_in_limbs|fast_heart_rate|pain_during_bowel_movements|pain_in_anal_region|bloody_stool|irritation_in_anus|neck_pain|dizziness|cramps|bruising|obesity|swollen_legs|swollen_blood_vessels|puffy_face_and_eyes|enlarged_thyroid|brittle_nails|swollen_extremeties|excessive_hunger|extra_marital_contacts|drying_and_tingling_lips|slurred_speech|knee_pain|hip_joint_pain|muscle_weakness|stiff_neck|swelling_joints|movement_stiffness|spinning_movements|loss_of_balance|unsteadiness|weakness_of_one_body_side|loss_of_smell|bladder_discomfort|foul_smell_of urine|continuous_feel_of_urine|passage_of_gases|internal_itching|toxic_look_(typhos)|depression|irritability|muscle_pain|altered_sensorium|red_spots_over_body|belly_pain|abnormal_menstruation|dischromic _patches|watering_from_eyes|increased_appetite|polyuria|family_history|mucoid_sputum|rusty_sputum|lack_of_concentration|visual_disturbances|receiving_blood_transfusion|receiving_unsterile_injections|coma|stomach_bleeding|distention_of_abdomen|history_of_alcohol_consumption|fluid_overload|blood_in_sputum|prominent_veins_on_calf|palpitations|painful_walking|pus_filled_pimples|blackheads|scurring|skin_peeling|silver_like_dusting|small_dents_in_nails|inflammatory_nails|blister|red_sore_around_nose|yellow_crust_ooze|       prognosis|\n",
      "+-------+---------+--------------------+-------------------+---------+------+----------+------------+-------+----------------+--------------+--------+-------------------+-------------------+-------+-----------+-------+--------------------+-----------+-----------+------------+--------+-----------------+---------------------+-----+----------+-----------+--------------+--------+-----------+-----------+--------+--------------+----------+------+----------------+--------------------+---------+------------+--------------+---------+----------+------------+-----------------+-------------------+-------------------+-------------------+-------+----------------------------+------+-----------------+---------------+--------------+----------+----------+----------+-----------------+---------------+---------------------------+-------------------+------------+------------------+---------+---------+------+--------+-------+------------+---------------------+-------------------+----------------+-------------+-------------------+----------------+----------------------+------------------------+--------------+---------+--------------+---------------+----------+---------------+------------------+------------------+---------------+------------+-------------------------+-------------+------------------+-------------------+------------------------+----------------+----------------+-------------------+----------+------------+-----------+-----------------+-------------------+----------+---------------------+-------------------+------------------+------------------+--------+--------------+-------------+------------+---------------------+-------------------+---------------------------+------------------------------+----+----------------+---------------------+------------------------------+--------------+---------------+-----------------------+------------+---------------+------------------+----------+--------+------------+-------------------+--------------------+------------------+-------+--------------------+-----------------+----------------+\n",
      "|      1|        1|                   1|                  0|        0|     0|         0|           0|      0|               0|             0|       0|                  0|                  0|      0|          0|      0|                   0|          0|          0|           0|       0|                0|                    0|    0|         0|          0|             0|       0|          0|          0|       0|             0|         0|     0|               0|                   0|        0|           0|             0|        0|         0|           0|                0|                  0|                  0|                  0|      0|                           0|     0|                0|              0|             0|         0|         0|         0|                0|              0|                          0|                  0|           0|                 0|        0|        0|     0|       0|      0|           0|                    0|                  0|               0|            0|                  0|               0|                     0|                       0|             0|        0|             0|              0|         0|              0|                 0|                 0|              0|           0|                        0|            0|                 0|                  0|                       0|               0|               0|                  0|         0|           0|          0|                0|                  0|         0|                    0|                  1|                 0|                 0|       0|             0|            0|           0|                    0|                  0|                          0|                             0|   0|               0|                    0|                             0|             0|              0|                      0|           0|              0|                 0|         0|       0|           0|                  0|                   0|                 0|      0|                   0|                0|Fungal infection|\n",
      "|      0|        1|                   1|                  0|        0|     0|         0|           0|      0|               0|             0|       0|                  0|                  0|      0|          0|      0|                   0|          0|          0|           0|       0|                0|                    0|    0|         0|          0|             0|       0|          0|          0|       0|             0|         0|     0|               0|                   0|        0|           0|             0|        0|         0|           0|                0|                  0|                  0|                  0|      0|                           0|     0|                0|              0|             0|         0|         0|         0|                0|              0|                          0|                  0|           0|                 0|        0|        0|     0|       0|      0|           0|                    0|                  0|               0|            0|                  0|               0|                     0|                       0|             0|        0|             0|              0|         0|              0|                 0|                 0|              0|           0|                        0|            0|                 0|                  0|                       0|               0|               0|                  0|         0|           0|          0|                0|                  0|         0|                    0|                  1|                 0|                 0|       0|             0|            0|           0|                    0|                  0|                          0|                             0|   0|               0|                    0|                             0|             0|              0|                      0|           0|              0|                 0|         0|       0|           0|                  0|                   0|                 0|      0|                   0|                0|Fungal infection|\n",
      "|      1|        0|                   1|                  0|        0|     0|         0|           0|      0|               0|             0|       0|                  0|                  0|      0|          0|      0|                   0|          0|          0|           0|       0|                0|                    0|    0|         0|          0|             0|       0|          0|          0|       0|             0|         0|     0|               0|                   0|        0|           0|             0|        0|         0|           0|                0|                  0|                  0|                  0|      0|                           0|     0|                0|              0|             0|         0|         0|         0|                0|              0|                          0|                  0|           0|                 0|        0|        0|     0|       0|      0|           0|                    0|                  0|               0|            0|                  0|               0|                     0|                       0|             0|        0|             0|              0|         0|              0|                 0|                 0|              0|           0|                        0|            0|                 0|                  0|                       0|               0|               0|                  0|         0|           0|          0|                0|                  0|         0|                    0|                  1|                 0|                 0|       0|             0|            0|           0|                    0|                  0|                          0|                             0|   0|               0|                    0|                             0|             0|              0|                      0|           0|              0|                 0|         0|       0|           0|                  0|                   0|                 0|      0|                   0|                0|Fungal infection|\n",
      "|      1|        1|                   0|                  0|        0|     0|         0|           0|      0|               0|             0|       0|                  0|                  0|      0|          0|      0|                   0|          0|          0|           0|       0|                0|                    0|    0|         0|          0|             0|       0|          0|          0|       0|             0|         0|     0|               0|                   0|        0|           0|             0|        0|         0|           0|                0|                  0|                  0|                  0|      0|                           0|     0|                0|              0|             0|         0|         0|         0|                0|              0|                          0|                  0|           0|                 0|        0|        0|     0|       0|      0|           0|                    0|                  0|               0|            0|                  0|               0|                     0|                       0|             0|        0|             0|              0|         0|              0|                 0|                 0|              0|           0|                        0|            0|                 0|                  0|                       0|               0|               0|                  0|         0|           0|          0|                0|                  0|         0|                    0|                  1|                 0|                 0|       0|             0|            0|           0|                    0|                  0|                          0|                             0|   0|               0|                    0|                             0|             0|              0|                      0|           0|              0|                 0|         0|       0|           0|                  0|                   0|                 0|      0|                   0|                0|Fungal infection|\n",
      "|      1|        1|                   1|                  0|        0|     0|         0|           0|      0|               0|             0|       0|                  0|                  0|      0|          0|      0|                   0|          0|          0|           0|       0|                0|                    0|    0|         0|          0|             0|       0|          0|          0|       0|             0|         0|     0|               0|                   0|        0|           0|             0|        0|         0|           0|                0|                  0|                  0|                  0|      0|                           0|     0|                0|              0|             0|         0|         0|         0|                0|              0|                          0|                  0|           0|                 0|        0|        0|     0|       0|      0|           0|                    0|                  0|               0|            0|                  0|               0|                     0|                       0|             0|        0|             0|              0|         0|              0|                 0|                 0|              0|           0|                        0|            0|                 0|                  0|                       0|               0|               0|                  0|         0|           0|          0|                0|                  0|         0|                    0|                  0|                 0|                 0|       0|             0|            0|           0|                    0|                  0|                          0|                             0|   0|               0|                    0|                             0|             0|              0|                      0|           0|              0|                 0|         0|       0|           0|                  0|                   0|                 0|      0|                   0|                0|Fungal infection|\n",
      "+-------+---------+--------------------+-------------------+---------+------+----------+------------+-------+----------------+--------------+--------+-------------------+-------------------+-------+-----------+-------+--------------------+-----------+-----------+------------+--------+-----------------+---------------------+-----+----------+-----------+--------------+--------+-----------+-----------+--------+--------------+----------+------+----------------+--------------------+---------+------------+--------------+---------+----------+------------+-----------------+-------------------+-------------------+-------------------+-------+----------------------------+------+-----------------+---------------+--------------+----------+----------+----------+-----------------+---------------+---------------------------+-------------------+------------+------------------+---------+---------+------+--------+-------+------------+---------------------+-------------------+----------------+-------------+-------------------+----------------+----------------------+------------------------+--------------+---------+--------------+---------------+----------+---------------+------------------+------------------+---------------+------------+-------------------------+-------------+------------------+-------------------+------------------------+----------------+----------------+-------------------+----------+------------+-----------+-----------------+-------------------+----------+---------------------+-------------------+------------------+------------------+--------+--------------+-------------+------------+---------------------+-------------------+---------------------------+------------------------------+----+----------------+---------------------+------------------------------+--------------+---------------+-----------------------+------------+---------------+------------------+----------+--------+------------+-------------------+--------------------+------------------+-------+--------------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Identify columns to drop\n",
    "columns_to_drop = []\n",
    "\n",
    "for cols in df.columns:\n",
    "    # Skip the 'prognosis' column (label column)\n",
    "    if cols == 'prognosis':\n",
    "        continue\n",
    "    \n",
    "    # Get distinct values in the column\n",
    "    unique_values = [row[0] for row in df.select(cols).distinct().collect()]\n",
    "    \n",
    "    # Check if the column has exactly two unique values and they are [0, 1]\n",
    "    if len(unique_values) != 2 or set(unique_values) != {0, 1}:\n",
    "        columns_to_drop.append(cols)\n",
    "\n",
    "# Drop unwanted columns\n",
    "df = df.drop(*columns_to_drop)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "print(f\"Dropped columns: {columns_to_drop}\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No columns with null values found.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count null values for each column\n",
    "null_counts = df.select(\n",
    "    [sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]\n",
    ")\n",
    "\n",
    "# Collect the null counts as a dictionary\n",
    "null_columns = null_counts.first().asDict()\n",
    "\n",
    "# Filter columns with non-zero null counts\n",
    "null_columns_with_counts = {col_name: count for col_name, count in null_columns.items() if count > 0}\n",
    "\n",
    "# Print results\n",
    "if null_columns_with_counts:\n",
    "    print(\"Columns with null values and their counts:\")\n",
    "    for col_name, count in null_columns_with_counts.items():\n",
    "        print(f\"{col_name}: {count}\")\n",
    "else:\n",
    "    print(\"No columns with null values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [cols for cols in df.columns if cols !='prognosis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming 'features' is a list of feature column names\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cols \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m----> 9\u001b[0m     frequency[cols] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Sort the frequency dictionary in descending order\u001b[39;00m\n\u001b[0;32m     12\u001b[0m sorted_frequency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28msorted\u001b[39m(frequency\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1264\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m   1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[1;32m-> 1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43m_load_from_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatchedSerializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCPickleSerializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:265\u001b[0m, in \u001b[0;36m_load_from_socket\u001b[1;34m(sock_info, serializer)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_socket\u001b[39m(sock_info: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaArray\u001b[39m\u001b[38;5;124m\"\u001b[39m, serializer: Serializer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Any]:\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    Connect to a local socket described by sock_info and use the given serializer to yield data\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m    usually a generator that yields deserialized data\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     sockfile \u001b[38;5;241m=\u001b[39m \u001b[43m_create_local_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# The socket will be automatically closed when garbage-collected.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m serializer\u001b[38;5;241m.\u001b[39mload_stream(sockfile)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:242\u001b[0m, in \u001b[0;36m_create_local_socket\u001b[1;34m(sock_info)\u001b[0m\n\u001b[0;32m    240\u001b[0m port: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m sock_info[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    241\u001b[0m auth_secret: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m sock_info[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 242\u001b[0m sockfile, sock \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_connect_and_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_secret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# The RDD materialization time is unpredictable, if we set a timeout for socket reading\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# operation, it will very possibly fail. See SPARK-18281.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m sock\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\java_gateway.py:211\u001b[0m, in \u001b[0;36mlocal_connect_and_auth\u001b[1;34m(port, auth_secret)\u001b[0m\n\u001b[0;32m    209\u001b[0m sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39msocket(af, socktype, proto)\n\u001b[0;32m    210\u001b[0m sock\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_AUTH_SOCKET_TIMEOUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m15\u001b[39m)))\n\u001b[1;32m--> 211\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m sockfile \u001b[38;5;241m=\u001b[39m sock\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrwb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_BUFFER_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m65536\u001b[39m)))\n\u001b[0;32m    213\u001b[0m _do_server_auth(sockfile, auth_secret)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dictionary to store frequencies\n",
    "frequency = {}\n",
    "\n",
    "# Assuming 'features' is a list of feature column names\n",
    "for cols in features:\n",
    "    frequency[cols] = df.select(sum(col(cols)).alias(\"frequency\")).collect()[0][\"frequency\"]\n",
    "\n",
    "# Sort the frequency dictionary in descending order\n",
    "sorted_frequency = dict(sorted(frequency.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Extract column names and their corresponding frequencies\n",
    "columns = list(sorted_frequency.keys())\n",
    "frequencies = list(sorted_frequency.values())\n",
    "\n",
    "# Plot the frequencies as a horizontal bar chart\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.barh(columns, frequencies)\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to show the highest frequency at the top\n",
    "plt.title(\"Frequency of Symptoms\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Symptoms\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o7602.pandasStructHandlingMode. Trace:\npy4j.Py4JException: Method pandasStructHandlingMode([]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\r\n\tat py4j.Gateway.invoke(Gateway.java:274)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     combined_df \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39munion(df_part)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Collect the aggregated data to the driver as a Pandas DataFrame\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Directory to save plots\u001b[39;00m\n\u001b[0;32m     26\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprognosis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:212\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    211\u001b[0m     timezone \u001b[38;5;241m=\u001b[39m jconf\u001b[38;5;241m.\u001b[39msessionLocalTimeZone()\n\u001b[1;32m--> 212\u001b[0m     struct_in_pandas \u001b[38;5;241m=\u001b[39m \u001b[43mjconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandasStructHandlingMode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[0;32m    215\u001b[0m         [\n\u001b[0;32m    216\u001b[0m             _create_converter_to_pandas(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    229\u001b[0m     )\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling o7602.pandasStructHandlingMode. Trace:\npy4j.Py4JException: Method pandasStructHandlingMode([]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\r\n\tat py4j.Gateway.invoke(Gateway.java:274)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, lit\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Ensure Spark DataFrame `df` and feature list `features` are defined\n",
    "# Aggregate symptom frequencies for all prognoses in one Spark job\n",
    "aggregated_data = []\n",
    "for symptom in features:  # Assuming 'features' contains the symptom columns\n",
    "    symptom_df = (\n",
    "        df.groupBy(\"prognosis\")\n",
    "        .agg(sum(col(symptom)).alias(\"frequency\"))\n",
    "        .withColumn(\"Symptom\", lit(symptom))\n",
    "    )\n",
    "    aggregated_data.append(symptom_df)\n",
    "\n",
    "# Combine all aggregated data into a single DataFrame\n",
    "combined_df = aggregated_data[0]\n",
    "for df_part in aggregated_data[1:]:\n",
    "    combined_df = combined_df.union(df_part)\n",
    "\n",
    "# Collect the aggregated data to the driver as a Pandas DataFrame\n",
    "result_df = combined_df.toPandas()\n",
    "\n",
    "# Directory to save plots\n",
    "output_dir = \"prognosis\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Process and save plots for each prognosis\n",
    "for prognosis in result_df[\"prognosis\"].unique():\n",
    "    prognosis_data = result_df[result_df[\"prognosis\"] == prognosis]\n",
    "    sorted_data = prognosis_data.sort_values(by=\"frequency\", ascending=False).head(5)\n",
    "\n",
    "    # Plot the horizontal bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(sorted_data[\"Symptom\"], sorted_data[\"frequency\"], color=\"skyblue\")\n",
    "    plt.gca().invert_yaxis()  # Highest frequency at the top\n",
    "    plt.title(f\"Top 5 Symptoms for Prognosis: {prognosis}\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Symptoms\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot in the specified folder\n",
    "    file_path = os.path.join(output_dir, f\"{prognosis}_top5_symptoms.png\")\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()  # Close the plot to avoid displaying it during loop execution\n",
    "\n",
    "print(f\"Plots saved in the '{output_dir}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(vertigo) Paroymsal  Positional Vertigo', 'AIDS', 'Acne', 'Alcoholic hepatitis', 'Allergy', 'Arthritis', 'Bronchial Asthma', 'Cervical spondylosis', 'Chicken pox', 'Chronic cholestasis', 'Common Cold', 'Dengue', 'Diabetes ', 'Dimorphic hemmorhoids(piles)', 'Drug Reaction', 'Fungal infection', 'GERD', 'Gastroenteritis', 'Heart attack', 'Hepatitis B', 'Hepatitis C', 'Hepatitis D', 'Hepatitis E', 'Hypertension ', 'Hyperthyroidism', 'Hypoglycemia', 'Hypothyroidism', 'Impetigo', 'Jaundice', 'Malaria', 'Migraine', 'Osteoarthristis', 'Paralysis (brain hemorrhage)', 'Peptic ulcer diseae', 'Pneumonia', 'Psoriasis', 'Tuberculosis', 'Typhoid', 'Urinary tract infection', 'Varicose veins', 'hepatitis A']\n"
     ]
    }
   ],
   "source": [
    "# turn label column to numeric\n",
    "labelIndexer = StringIndexer(inputCol=\"prognosis\", outputCol=\"label\")\n",
    "indexer_label = labelIndexer.fit(df)\n",
    "df = indexer_label.transform(df)\n",
    "\n",
    "\n",
    "df = df.drop('prognosis')\n",
    "\n",
    "train , test = df.randomSplit([0.8,0.2],seed=1)\n",
    "\n",
    "print(indexer_label.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric: 0, Label: (vertigo) Paroymsal  Positional Vertigo\n",
      "Numeric: 1, Label: AIDS\n",
      "Numeric: 2, Label: Acne\n",
      "Numeric: 3, Label: Alcoholic hepatitis\n",
      "Numeric: 4, Label: Allergy\n",
      "Numeric: 5, Label: Arthritis\n",
      "Numeric: 6, Label: Bronchial Asthma\n",
      "Numeric: 7, Label: Cervical spondylosis\n",
      "Numeric: 8, Label: Chicken pox\n",
      "Numeric: 9, Label: Chronic cholestasis\n",
      "Numeric: 10, Label: Common Cold\n",
      "Numeric: 11, Label: Dengue\n",
      "Numeric: 12, Label: Diabetes \n",
      "Numeric: 13, Label: Dimorphic hemmorhoids(piles)\n",
      "Numeric: 14, Label: Drug Reaction\n",
      "Numeric: 15, Label: Fungal infection\n",
      "Numeric: 16, Label: GERD\n",
      "Numeric: 17, Label: Gastroenteritis\n",
      "Numeric: 18, Label: Heart attack\n",
      "Numeric: 19, Label: Hepatitis B\n",
      "Numeric: 20, Label: Hepatitis C\n",
      "Numeric: 21, Label: Hepatitis D\n",
      "Numeric: 22, Label: Hepatitis E\n",
      "Numeric: 23, Label: Hypertension \n",
      "Numeric: 24, Label: Hyperthyroidism\n",
      "Numeric: 25, Label: Hypoglycemia\n",
      "Numeric: 26, Label: Hypothyroidism\n",
      "Numeric: 27, Label: Impetigo\n",
      "Numeric: 28, Label: Jaundice\n",
      "Numeric: 29, Label: Malaria\n",
      "Numeric: 30, Label: Migraine\n",
      "Numeric: 31, Label: Osteoarthristis\n",
      "Numeric: 32, Label: Paralysis (brain hemorrhage)\n",
      "Numeric: 33, Label: Peptic ulcer diseae\n",
      "Numeric: 34, Label: Pneumonia\n",
      "Numeric: 35, Label: Psoriasis\n",
      "Numeric: 36, Label: Tuberculosis\n",
      "Numeric: 37, Label: Typhoid\n",
      "Numeric: 38, Label: Urinary tract infection\n",
      "Numeric: 39, Label: Varicose veins\n",
      "Numeric: 40, Label: hepatitis A\n"
     ]
    }
   ],
   "source": [
    "# Show the mapping of labels and their corresponding numeric values\n",
    "label_mapping = list(enumerate(indexer_label.labels))\n",
    "for label_number, label_name in label_mapping:\n",
    "    print(f\"Numeric: {label_number}, Label: {label_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelComparisonPipeline:\n",
    "    def __init__(self, spark_session, train, test, top_features: list, label_column: str):\n",
    "        self.spark = spark_session\n",
    "        self.models = []  # Store models for comparison\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.top_features = top_features\n",
    "        self.label_column = label_column\n",
    "\n",
    "    def _build_pipeline(self, model):\n",
    "        # Step 1: Assemble features\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=self.top_features, outputCol=\"assembled_features\"\n",
    "        )\n",
    "        model.setFeaturesCol(\"assembled_features\")\n",
    "        # Step 2: Build pipeline with assembler and model\n",
    "        pipeline = Pipeline(stages=[assembler, model])\n",
    "        return pipeline\n",
    "\n",
    "    def compare_models(self, models_with_params):\n",
    "        # Split the data into training and testing sets\n",
    "        train_df, test_df = self.train, self.test\n",
    "\n",
    "        # Initialize evaluator\n",
    "        evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol=self.label_column, predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "        )\n",
    "\n",
    "        # Iterate through models and evaluate\n",
    "        results = []\n",
    "        for model_name, model, param_grid in models_with_params:\n",
    "            print(f\"Training and evaluating {model_name}...\")\n",
    "\n",
    "            # Build pipeline with the current model\n",
    "            # pipeline = self._build_pipeline(model)\n",
    "            pipeline = Pipeline(stages=[model])\n",
    "\n",
    "            # Train the model\n",
    "            trained_model = pipeline.fit(train_df)\n",
    "\n",
    "            # Evaluate the model\n",
    "            predictions = trained_model.transform(test_df)\n",
    "            accuracy = evaluator.evaluate(predictions)\n",
    "            print(f\"{model_name} Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "            # Store results\n",
    "            results.append((model_name, accuracy, trained_model))\n",
    "            #    Define CrossValidator with the model and its parameter grid\n",
    "            # cv = CrossValidator(\n",
    "            #     estimator=pipeline,\n",
    "            #     estimatorParamMaps=param_grid,\n",
    "            #     evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\"),\n",
    "            #     numFolds=3  # Adjust the number of folds as needed\n",
    "            # )\n",
    "\n",
    "            # # Fit the model and evaluate\n",
    "            # cv_model = cv.fit(train)\n",
    "            # accuracy = cv_model.avgMetrics[0]  # Access accuracy from CrossValidator\n",
    "\n",
    "        # Save results\n",
    "        results.append((model_name, accuracy, trained_model))\n",
    "        # Return sorted results by accuracy\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, NaiveBayes\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(featuresaCol=\"features\", labelCol=\"label\", maxBins=45)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", maxBins=45)\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "train = assembler.transform(train)\n",
    "test = assembler.transform(test)\n",
    "\n",
    "# Parameter Grid for Decision Tree\n",
    "dt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(dt.maxBins, [10, 20, 40]) \\\n",
    "    .build()\n",
    "\n",
    "# Parameter Grid for Random Forest\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "# Parameter Grid for Naive Bayes\n",
    "nb_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0.5, 1.0, 2.0]) \\\n",
    "    .build()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all models and parameter grids for cross-validation\n",
    "models_with_params = [\n",
    "    (\"Decision Tree\", dt, dt_param_grid),\n",
    "    (\"Random Forest\", rf, rf_param_grid),\n",
    "    (\"Naive Bayes\", nb, nb_param_grid)\n",
    "]\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Decision Tree...\n",
      "Average evaluator metrics for each parameter combination:\n",
      "Parameter combination 1: 0.061469839955447705\n",
      "Parameter combination 2: 0.061469839955447705\n",
      "Parameter combination 3: 0.061469839955447705\n",
      "Parameter combination 4: 0.12046167237025973\n",
      "Parameter combination 5: 0.12046167237025973\n",
      "Parameter combination 6: 0.12046167237025973\n",
      "Parameter combination 7: 0.2403541153654377\n",
      "Parameter combination 8: 0.2403541153654377\n",
      "Parameter combination 9: 0.2403541153654377\n",
      "Best model saved to E:/third year/Big_Data_Tech/Project/Disease_prediction/models/Decision Tree\n",
      "Training and evaluating Random Forest...\n",
      "Average evaluator metrics for each parameter combination:\n",
      "Parameter combination 1: 0.8935312152333041\n",
      "Parameter combination 2: 0.9926672386673951\n",
      "Parameter combination 3: 1.0\n",
      "Parameter combination 4: 0.9753036109376119\n",
      "Parameter combination 5: 1.0\n",
      "Parameter combination 6: 1.0\n",
      "Best model saved to E:/third year/Big_Data_Tech/Project/Disease_prediction/models/Random Forest\n",
      "Training and evaluating Naive Bayes...\n",
      "Average evaluator metrics for each parameter combination:\n",
      "Parameter combination 1: 1.0\n",
      "Parameter combination 2: 1.0\n",
      "Parameter combination 3: 1.0\n",
      "Best model saved to E:/third year/Big_Data_Tech/Project/Disease_prediction/models/Naive Bayes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, lit\n",
    "import pandas as pd\n",
    "\n",
    "train.cache()\n",
    "\n",
    "# Compare models\n",
    "best_models = []\n",
    "\n",
    "for model_name, model, param_grid in models_with_params:\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "\n",
    "    # Define pipeline\n",
    "    pipeline = Pipeline(stages=[model])\n",
    "\n",
    "    # Define CrossValidator\n",
    "    cv = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=5  # Adjust number of folds as needed\n",
    "    )\n",
    "\n",
    "    # Fit model using CrossValidator\n",
    "    cv_model = cv.fit(train)\n",
    "\n",
    "    # Get best model\n",
    "    best_model = cv_model.bestModel\n",
    "    avgMetrics = cv_model.avgMetrics\n",
    "    # Print the average metrics\n",
    "    print(\"Average evaluator metrics for each parameter combination:\")\n",
    "    for i, metric in enumerate(avgMetrics):\n",
    "        print(f\"Parameter combination {i + 1}: {metric}\")\n",
    "\n",
    "    # Specify the path to save the best model\n",
    "    # output_path = f\"hdfs://localhost:9000/usr/baonguyen/models/{model_name}\"\n",
    "    output_path = f\"E:/third year/Big_Data_Tech/Project/Disease_prediction/models1/{model_name}\"\n",
    "\n",
    "    # Save the best model\n",
    "    best_model.write().overwrite().save(output_path)\n",
    "    print(f\"Best model saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the VectorAssembler to assemble top features\n",
    "assembler = VectorAssembler(inputCols=top_features, outputCol=\"features\")\n",
    "\n",
    "# Prepare the final DataFrame\n",
    "df = assembler.transform(df).select(\"features\", \"prognosis\")  # Ensure target column is 'prognosis'\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define the RandomForest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"prognosis\", predictionCol=\"prediction\")\n",
    "\n",
    "# Hyperparameter grid for Cross-Validation\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Evaluator for classification\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"prognosis\", metricName=\"accuracy\")\n",
    "\n",
    "# CrossValidator for hyperparameter tuning\n",
    "cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,  # 5-fold cross-validation\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "# Pipeline to assemble and train\n",
    "pipeline = Pipeline(stages=[cv])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the RandomForest model...\")\n",
    "cv_model = pipeline.fit(train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = cv_model.transform(test)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "model_output_path = \"/content/drive/MyDrive/Colab Notebooks/Big data/Random_Forest_TopFeatures\"\n",
    "cv_model.bestModel.write().overwrite().save(model_output_path)\n",
    "print(f\"Model saved at: {model_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see that naive bayes and random forest did really well using cross validation then we use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6019.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6019.0 (TID 5343) (DESKTOP-P3USJPK executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/third year/Big_Data_Tech/Project/Disease_prediction/models/Naive Bayes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the pipeline model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m pipeline_model \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Make predictions on the test dataset\u001b[39;00m\n\u001b[0;32m      8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m pipeline_model\u001b[38;5;241m.\u001b[39mtransform(test)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\pipeline.py:282\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelineModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 282\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(cast(Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaMLReadable[PipelineModel]\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls))\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\util.py:579\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[1;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m metadataPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m metadataStr \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadedVals\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6019.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6019.0 (TID 5343) (DESKTOP-P3USJPK executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "# model_path = \"hdfs://localhost:9000/usr/baonguyen/models/Naive Bayes\"\n",
    "model_path = \"E:/third year/Big_Data_Tech/Project/Disease_prediction/models/Naive Bayes\"\n",
    "# Load the pipeline model\n",
    "pipeline_model = PipelineModel.load(model_path)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = pipeline_model.transform(test)\n",
    "# Ensure the folder exists\n",
    "metrics_folder = \"metrics\"\n",
    "os.makedirs(metrics_folder, exist_ok=True)\n",
    "# Evaluate on test data\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"{model_name} Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Compute precision and recall for each class\n",
    "classes = predictions.select(\"label\").distinct().orderBy(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "class_labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for cls in classes:\n",
    "    # Precision\n",
    "    precision_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"precisionByLabel\"\n",
    "    )\n",
    "    precision = precision_evaluator.evaluate(predictions, {precision_evaluator.metricLabel: cls})\n",
    "\n",
    "    # Recall\n",
    "    recall_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"recallByLabel\"\n",
    "    )\n",
    "    recall = recall_evaluator.evaluate(predictions, {recall_evaluator.metricLabel: cls})\n",
    "\n",
    "    class_labels.append(cls)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Create a DataFrame for class-specific metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Class\": class_labels,\n",
    "    \"Precision\": precisions,\n",
    "    \"Recall\": recalls\n",
    "})\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_file = os.path.join(metrics_folder, f\"{model_name}_metrics.csv\")\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"Class-specific metrics saved to: {metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.csv('hdfs://localhost:9000/usr/baonguyen/datatest.csv',header=True,inferSchema=True)\n",
    "df_test = spark.read.csv('hdfs://localhost:9000/usr/baonguyen/datatest.csv',header=True,inferSchema=True)\n",
    "df_test = df_test.drop(*columns_to_drop)\n",
    "df_test = indexer_label.transform(df_test)\n",
    "test_data = df_test\n",
    "test_data = assembler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------+-----+-----------------+----------------------------+\n",
      "|features                                                                                                |label|predicted_numeric|predicted_label             |\n",
      "+--------------------------------------------------------------------------------------------------------+-----+-----------------+----------------------------+\n",
      "|(131,[0,1,2,101],[1.0,1.0,1.0,1.0])                                                                     |15.0 |15.0             |Fungal infection            |\n",
      "|(131,[3,4,5,102],[1.0,1.0,1.0,1.0])                                                                     |4.0  |4.0              |Allergy                     |\n",
      "|(131,[7,8,9,11,24,55],[1.0,1.0,1.0,1.0,1.0,1.0])                                                        |16.0 |16.0             |GERD                        |\n",
      "|(131,[0,11,32,34,35,39,43],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                               |9.0  |9.0              |Chronic cholestasis         |\n",
      "|(131,[0,1,7,12,13],[1.0,1.0,1.0,1.0,1.0])                                                               |14.0 |14.0             |Drug Reaction               |\n",
      "|(131,[11,30,35,39,91,92],[1.0,1.0,1.0,1.0,1.0,1.0])                                                     |33.0 |33.0             |Peptic ulcer diseae         |\n",
      "|(131,[10,22,25,74],[1.0,1.0,1.0,1.0])                                                                   |1.0  |1.0              |AIDS                        |\n",
      "|(131,[14,19,20,21,23,48,66,73,103,104],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                       |12.0 |12.0             |Diabetes                    |\n",
      "|(131,[11,26,29,40],[1.0,1.0,1.0,1.0])                                                                   |17.0 |17.0             |Gastroenteritis             |\n",
      "|(131,[14,24,25,27,105,106],[1.0,1.0,1.0,1.0,1.0,1.0])                                                   |6.0  |6.0              |Bronchial Asthma            |\n",
      "|(131,[31,55,63,84,108],[1.0,1.0,1.0,1.0,1.0])                                                           |23.0 |23.0             |Hypertension                |\n",
      "|(131,[8,30,31,48,73,80,94,95,109],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                |30.0 |30.0             |Migraine                    |\n",
      "|(131,[37,56,62,63,84],[1.0,1.0,1.0,1.0,1.0])                                                            |7.0  |7.0              |Cervical spondylosis        |\n",
      "|(131,[11,31,86,97],[1.0,1.0,1.0,1.0])                                                                   |32.0 |32.0             |Paralysis (brain hemorrhage)|\n",
      "|(131,[0,11,14,19,25,32,33,39],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                        |28.0 |28.0             |Jaundice                    |\n",
      "|(131,[5,11,25,28,31,34,40,96],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                        |29.0 |29.0             |Malaria                     |\n",
      "|(131,[0,1,14,21,25,31,35,41,46,47,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                    |8.0  |8.0              |Chicken pox                 |\n",
      "|(131,[1,5,6,11,14,25,31,34,35,36,37,47,96,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|11.0 |11.0             |Dengue                      |\n",
      "|(131,[5,11,14,25,31,34,38,39,40,93,99],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |37.0 |37.0             |Typhoid                     |\n",
      "|(131,[6,11,32,33,34,35,39,40,41,43,96],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |40.0 |40.0             |hepatitis A                 |\n",
      "+--------------------------------------------------------------------------------------------------------+-----+-----------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming you have a trained model and a test dataset\n",
    "# Replace 'model' with your trained model and 'test' with your test dataset\n",
    "\n",
    "model_path = \"hdfs://localhost:9000/usr/baonguyen/models/Naive Bayes\"\n",
    "\n",
    "\n",
    "predictions = pipeline_model.transform(test_data)\n",
    "\n",
    "\n",
    "\n",
    "# Show predictions with numeric and original labels\n",
    "# Use the label mapping to convert numeric predictions back to original labels\n",
    "predictions_with_labels = predictions.select(\n",
    "    col(\"features\"),\n",
    "    col('label'),\n",
    "    col(\"prediction\").alias(\"predicted_numeric\"),\n",
    "    col(\"probability\").alias(\"predicted_probabilities\")\n",
    ")\n",
    "\n",
    "# Map numeric predictions back to original labels\n",
    "label_mapping = list(enumerate(indexer_label.labels))\n",
    "numeric_to_label = {num: label for num, label in label_mapping}\n",
    "\n",
    "# Convert predicted numeric to original labels\n",
    "def map_numeric_to_label(numeric_value):\n",
    "    return numeric_to_label.get(int(numeric_value), \"Unknown\")\n",
    "\n",
    "# Register UDF for mapping\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "map_to_label_udf = udf(map_numeric_to_label, StringType())\n",
    "\n",
    "# Add original label to predictions\n",
    "final_predictions = predictions_with_labels.withColumn(\n",
    "    \"predicted_label\",\n",
    "    map_to_label_udf(col(\"predicted_numeric\"))\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "final_predictions.select(\"features\",'label' ,\"predicted_numeric\", \"predicted_label\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
